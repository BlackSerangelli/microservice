# ğŸ¤– Chat con DeepSeek Coder - Microservicio Ollama

Una aplicaciÃ³n web simple y rÃ¡pida para chatear con el modelo **DeepSeek Coder** a travÃ©s de la API de Ollama, especializada en programaciÃ³n y desarrollo de software.

## âœ¨ CaracterÃ­sticas

- ğŸš€ **Chat en tiempo real** con DeepSeek Coder
- âš¡ **Respuestas optimizadas** para mayor velocidad
- ğŸ’» **Especializado en programaciÃ³n** y desarrollo
- ğŸ¨ **Interfaz simple y funcional** 
- ğŸ“± **DiseÃ±o responsive** para mÃ³viles y escritorio
- ğŸ”— **ConexiÃ³n directa** a Ollama API
- ğŸ—‘ï¸ **FunciÃ³n para limpiar** el historial del chat

## ğŸ—ï¸ Arquitectura del Proyecto

```
microservicio01/
â”œâ”€â”€ app.py                 # Servidor Flask principal
â”œâ”€â”€ requirements.txt       # Dependencias de Python
â”œâ”€â”€ README.md             # Este archivo
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # PÃ¡gina HTML del chat
â””â”€â”€ static/
    â”œâ”€â”€ style.css         # Estilos CSS
    â””â”€â”€ script.js         # LÃ³gica JavaScript del chat
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

1. **Python 3.7+** instalado en tu sistema
2. **Ollama** ejecutÃ¡ndose en `localhost:11434`
3. **Modelo DeepSeek Coder** descargado en Ollama

### Pasos de instalaciÃ³n

1. **Clona o descarga** este proyecto
2. **Instala las dependencias**:
   ```bash
   pip3 install -r requirements.txt
   ```

3. **AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose**:
   ```bash
   # En otra terminal
   ollama serve
   ```

4. **Descarga el modelo DeepSeek Coder** (si no lo tienes):
   ```bash
   ollama pull deepseek-coder
   ```

## ğŸ¯ Uso

### Iniciar el microservicio

```bash
python3 app.py
```

El servicio estarÃ¡ disponible en:
- ğŸŒ **PÃ¡gina web**: http://localhost:5000
- ğŸ”— **API de chat**: http://localhost:5000/api/chat

### Usar la aplicaciÃ³n

1. **Abre tu navegador** y ve a http://localhost:5000
2. **Verifica el estado** de conexiÃ³n con Ollama
3. **Escribe tu pregunta** de programaciÃ³n en el campo de texto
4. **Presiona Enter** o haz clic en "Enviar"
5. **Recibe la respuesta** de DeepSeek Coder en tiempo real
6. **Usa "ğŸ—‘ï¸ Limpiar Chat"** para empezar una nueva conversaciÃ³n

## âš¡ Optimizaciones de Velocidad

El proyecto incluye varias optimizaciones para respuestas mÃ¡s rÃ¡pidas:

### ParÃ¡metros del modelo optimizados:
- **`temperature: 0.1`** - Respuestas mÃ¡s consistentes y rÃ¡pidas
- **`top_p: 0.1`** - Menor aleatoriedad = mayor velocidad
- **`top_k: 10`** - Menos opciones = respuestas mÃ¡s rÃ¡pidas
- **`num_predict: 100`** - Limita la longitud de respuesta
- **`repeat_penalty: 1.0`** - Evita repeticiones innecesarias

### Configuraciones del servidor:
- **Timeout reducido** de 30 a 15 segundos
- **Indicadores de carga** para mejor experiencia de usuario

## ğŸ“¡ API Endpoints

### GET /
PÃ¡gina principal del chat web.

### POST /api/chat
Proxy para la API de Ollama. EnvÃ­a un mensaje al modelo DeepSeek Coder.

**Request:**
```json
{
    "message": "Tu pregunta de programaciÃ³n aquÃ­"
}
```

**Response:**
```json
{
    "model": "deepseek-coder",
    "message": {
        "content": "Respuesta del modelo"
    },
    "usage": {...}
}
```

### GET /api/respuesta
Endpoint de prueba que retorna un mensaje simple.

## ğŸ”§ ConfiguraciÃ³n

### Cambiar el modelo de IA

Para usar un modelo diferente a DeepSeek Coder, modifica la variable en `app.py`:

```python
# En la funciÃ³n chat()
"model": "llama2"  # o cualquier otro modelo disponible
```

### Cambiar el puerto

Modifica la lÃ­nea final en `app.py`:

```python
app.run(host='0.0.0.0', port=8080)  # Puerto 8080
```

### Ajustar parÃ¡metros de velocidad

Modifica los parÃ¡metros en `app.py` para ajustar la velocidad vs. calidad:

```python
"options": {
    "temperature": 0.1,    # 0.0 = mÃ¡s rÃ¡pido, 1.0 = mÃ¡s creativo
    "top_p": 0.1,         # 0.0 = mÃ¡s rÃ¡pido, 1.0 = mÃ¡s variado
    "num_predict": 100,   # Menor = mÃ¡s rÃ¡pido, Mayor = respuestas mÃ¡s largas
}
```

## ğŸ› SoluciÃ³n de Problemas

### Error de conexiÃ³n con Ollama

1. **Verifica que Ollama estÃ© ejecutÃ¡ndose**:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. **AsegÃºrate de que el modelo estÃ© descargado**:
   ```bash
   ollama list
   ```

3. **Descarga el modelo si no estÃ¡ disponible**:
   ```bash
   ollama pull deepseek-coder
   ```

4. **Reinicia Ollama** si es necesario:
   ```bash
   pkill ollama
   ollama serve
   ```

### Puerto 5000 ocupado (macOS)

En macOS, el puerto 5000 puede estar ocupado por AirPlay. Solucion:

1. **Deshabilitar AirPlay Receiver**:
   - System Preferences â†’ Sharing â†’ AirPlay Receiver


### Problemas de dependencias

Si hay problemas con las dependencias:

```bash
pip3 install --upgrade pip
pip3 install -r requirements.txt --force-reinstall
```

## ğŸ’» TecnologÃ­as Utilizadas

- **Backend**: Flask (Python)
- **Frontend**: HTML5, CSS3, JavaScript (ES6+)
- **API**: Ollama REST API
- **Modelo de IA**: DeepSeek Coder
- **Estilos**: CSS personalizado con gradientes y animaciones

## ğŸ¨ PersonalizaciÃ³n

### Cambiar colores

Modifica las variables CSS en `static/style.css`:

```css
body {
    background: linear-gradient(135deg, #tu-color 0%, #otro-color 100%);
}
```

### Agregar funcionalidades

El archivo `static/script.js` estÃ¡ estructurado de manera modular. Puedes:

- Agregar nuevos tipos de mensajes
- Implementar historial de conversaciones
- Agregar exportaciÃ³n de chats
- Integrar con otras APIs

## ğŸ“± Compatibilidad

- âœ… **Chrome** 80+
- âœ… **Firefox** 75+
- âœ… **Safari** 13+
- âœ… **Edge** 80+
- âœ… **MÃ³viles** (iOS Safari, Chrome Mobile)

## ğŸ” Funciones Especiales

### VerificaciÃ³n automÃ¡tica de conexiÃ³n
La aplicaciÃ³n verifica automÃ¡ticamente:
- Estado de conexiÃ³n con Ollama
- Disponibilidad del modelo deepseek-coder

### Indicadores de estado
- ğŸŸ¢ **Verde**: Conectado a Ollama
- ğŸ”´ **Rojo**: Sin conexiÃ³n
- ğŸŸ¡ **Amarillo**: Error de conexiÃ³n

### FunciÃ³n de limpieza
- BotÃ³n "ğŸ—‘ï¸ Limpiar Chat" para reiniciar la conversaciÃ³n
- Ãštil para empezar nuevos temas o proyectos

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request


### Evidencia: Funcionamiento del LLm
![Evidencia 1](screenshots/evidencia1.png)

## ğŸ†˜ Soporte

Si tienes problemas o preguntas:

1. Revisa la secciÃ³n de soluciÃ³n de problemas
2. Verifica que Ollama estÃ© funcionando correctamente
3. AsegÃºrate de que el modelo deepseek-coder estÃ© descargado
4. Revisa los logs del servidor Flask
5. Abre un issue en el repositorio

## ğŸš€ Roadmap

### PrÃ³ximas funcionalidades:
- [ ] MÃºltiples modelos de IA

---

**Â¡Disfruta programando con DeepSeek Coder! ğŸš€ğŸ’»**

*Desarrollado con â¤ï¸ para la comunidad de desarrolladores* 