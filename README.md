# ğŸ¤– Chat con DeepSeek Coder - Web estÃ¡tica (proxy opcional)

Esta es una versiÃ³n ligera de la aplicaciÃ³n para chatear con el modelo **DeepSeek Coder** a travÃ©s de la API de Ollama.

Ahora la interfaz es puramente estÃ¡tica: `index.html` estÃ¡ en la raÃ­z del proyecto y la lÃ³gica cliente (`/static/script.js`) realiza las llamadas HTTP directamente a la API de Ollama (por defecto `http://localhost:11434`).

Si necesitas un proxy por restricciones CORS, hay un `app.py` de ejemplo en el repositorio; de lo contrario puedes servir la web directamente con un servidor estÃ¡tico.

## âœ… QuÃ© cambiÃ³

- `index.html` estÃ¡ en la raÃ­z del proyecto (ya no usamos `templates/`).
- Las llamadas al modelo se hacen desde el navegador hacia `http://localhost:11434/api/chat`.
- El servidor Flask ya no es necesario para servir la app; estÃ¡ incluido solo como referencia.

## ğŸ“ Estructura del proyecto

```
microservicio01/
â”œâ”€â”€ app.py                 # (opcional) proxy ejemplo â€” no necesario para la versiÃ³n estÃ¡tica
â”œâ”€â”€ requirements.txt       # Dependencias (solo si usas el proxy Flask)
â”œâ”€â”€ README.md              # Este archivo
â”œâ”€â”€ index.html             # PÃ¡gina HTML del chat (ahora en la raÃ­z)
â””â”€â”€ static/
   â”œâ”€â”€ style.css         # Estilos CSS
   â””â”€â”€ script.js         # LÃ³gica JavaScript del chat
```

## ğŸš€ Ejecutar localmente (rÃ¡pido)

1. Arranca Ollama y asegÃºrate de que el modelo estÃ© disponible:

```bash
ollama serve
ollama pull deepseek-coder
```

2. Sirve la aplicaciÃ³n estÃ¡tica desde la raÃ­z del proyecto:

```bash
python3 -m http.server 8080
```

3. Abre en el navegador:

- http://localhost:8080

Si prefieres mantener la URL antigua `/templates/index.html`, crea una carpeta `templates` con un `index.html` que redirija a `/index.html`.

## âš ï¸ Nota sobre CORS

Si el navegador bloquea las peticiones a `http://localhost:11434` por CORS verÃ¡s errores en la consola. Opciones:

- Habilitar CORS en Ollama (si estÃ¡ soportado).
- Ejecutar un proxy local que aÃ±ada los headers CORS.
- Usar el proxy `app.py` incluido (requiere Python/Flask).

## ğŸ§­ Uso

1. Verifica que Ollama estÃ© corriendo:

```bash
curl http://localhost:11434/api/tags
```

2. Escribe tu pregunta en la interfaz y presiona Enviar.

3. Usa "ğŸ—‘ï¸ Limpiar Chat" para reiniciar la conversaciÃ³n.

## ğŸ› SoluciÃ³n de Problemas

Si algo no carga (404):

- AsegÃºrate de iniciar el servidor estÃ¡tico desde la raÃ­z del proyecto.
- Verifica que `static/style.css` y `static/script.js` existan.
- Abre DevTools â†’ Network y comprueba las rutas que fallan.

Si la API da errores, revisa los logs de Ollama y el estado del modelo.

---

Si quieres que actualice otra parte del README o que elimine `app.py` por completo, lo hago.
